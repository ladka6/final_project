{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQphyvj7gJLM",
        "outputId": "28c6f161-21bb-43f5-dcf6-a35a8929f7e4"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRpxGoDRgCUi",
        "outputId": "c50bb74e-f1bf-4987-c12d-8dad83c2f4b8"
      },
      "outputs": [],
      "source": [
        "%pip install transformers\n",
        "%pip install datasets\n",
        "%pip install accelerate -U\n",
        "%pip install transformers[torch]\n",
        "%pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "Rf8GgYFbf8D9",
        "outputId": "f561385e-3050-45ba-e7a1-f06f013a3d8b"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments, TrainerCallback\n",
        "from datasets import load_dataset\n",
        "\n",
        "class CustomLoggingCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        if state.global_step % args.logging_steps == 0:\n",
        "            print(f\"Step: {state.global_step}, Loss: {logs['loss']}\")\n",
        "\n",
        "\n",
        "\n",
        "def load_dataset_from_csv(file_path, tokenizer, max_length=512):\n",
        "    dataset = load_dataset('csv', data_files=file_path, split='train')\n",
        "    dataset = dataset.filter(lambda example: all(value is not None for value in example.values()))\n",
        "    dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        inputs = tokenizer(examples['lang1'], padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "        targets = tokenizer(examples['lang2'], padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "        return {'input_ids': inputs.input_ids, 'labels': targets.input_ids}\n",
        "\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "    return tokenized_datasets\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
        "tokenized_dataset = load_dataset_from_csv(\"./drive/MyDrive/preprocessed.csv\", tokenizer)\n",
        "callback = CustomLoggingCallback()\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/codet5-finetuned\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    save_steps=600,  \n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    logging_steps=100,  \n",
        "    evaluation_strategy=\"steps\",  \n",
        "    eval_steps=200,  \n",
        "    logging_dir=\"./logs\",  \n",
        "    logging_first_step=True,  \n",
        "    load_best_model_at_end=True,  \n",
        "    metric_for_best_model=\"eval_loss\",  \n",
        "    greater_is_better=False,  \n",
        "    learning_rate=5e-5,  \n",
        "    lr_scheduler_type=\"linear\",  \n",
        "    warmup_steps=0,  \n",
        "    gradient_accumulation_steps=1,  \n",
        "    logging_strategy=\"steps\",  \n",
        ")\n",
        "\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, label_pad_token_id=-100)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['test'],\n",
        "    callbacks=[callback]\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae4rfaAqzF7T",
        "outputId": "01320020-e3d1-4ded-c426-bc3e6587f9ad"
      },
      "outputs": [],
      "source": [
        "def generate_python_code(java_code, model, tokenizer):\n",
        "    device = next(model.parameters()).device  # Get the device of the model\n",
        "    input_ids = tokenizer(java_code, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "    output = model.generate(input_ids=input_ids, max_length=100, num_return_sequences=1, early_stopping=True)\n",
        "    python_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return python_code\n",
        "\n",
        "# Example usage\n",
        "java_code = \"\"\"\n",
        "class GFG {\n",
        "    public static void main(String[] args)\n",
        "    {\n",
        "        // Declaring and initializing integer variable\n",
        "        int num = 10;\n",
        "        // Checking if number is even or odd number\n",
        "        if (num % 2 == 0) {\n",
        "            System.out.println(\"Entered Number is Even\");\n",
        "        }\n",
        "\n",
        "        else {\n",
        "            System.out.println(\"Entered Number is Odd\");\n",
        "\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\"\"\"\n",
        "python_code = generate_python_code(java_code, model, tokenizer)\n",
        "print(\"Generated Python code:\\n\", python_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Z0Jxc4i0zGyb",
        "outputId": "7285f204-5429-49ba-8cdb-82c9d2c02c81"
      },
      "outputs": [],
      "source": [
        "evaluation = trainer.evaluate()\n",
        "\n",
        "print(\"Evaluation results:\", evaluation)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
